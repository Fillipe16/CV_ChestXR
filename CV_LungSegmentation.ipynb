{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Instalando dependencias faltantes\n!pip install segmentation_models_pytorch\n!pip install pytorch_lightning\n!pip install imutils","metadata":{"id":"sURBAxWJas3Z","outputId":"9c7ce7e3-d4e5-4ece-8875-62e94b6d0cfc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Blibiotecas para manipulação de dados\nimport numpy as np\nfrom imutils import paths\nimport torch\nimport re\nfrom shutil import make_archive\n# Bibliotecas de visualização\nimport matplotlib.pyplot as plt\nimport PIL\nimport cv2\n# Bibliotecas para seleção\nfrom sklearn.model_selection import train_test_split\n# Bibliotecas de modelos\nimport segmentation_models_pytorch as sm\nimport pytorch_lightning as pl\n\nfrom torchmetrics import Dice\nfrom torchvision import transforms\n","metadata":{"id":"vbgLcjrsDenx","execution":{"iopub.status.busy":"2023-02-05T19:19:50.061157Z","iopub.execute_input":"2023-02-05T19:19:50.062080Z","iopub.status.idle":"2023-02-05T19:19:57.376293Z","shell.execute_reply.started":"2023-02-05T19:19:50.062043Z","shell.execute_reply":"2023-02-05T19:19:57.375213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1-) Leitura e Preprocessamento dos dados","metadata":{"id":"elj1ALO4Fa2M"}},{"cell_type":"code","source":"# Extraindo apenas os paths das imagens -- Montgomery Dataset\npath_montgomery = \"/kaggle/input/lungs-classification/NLM-MontgomeryCXRSet/MontgomerySet\"\nfolders = ['/CXR_png','/ManualMask/leftMask','/ManualMask/rightMask']\n\nimgs_paths_01 = sorted(list(paths.list_images(path_montgomery+folders[0])))\nleft_masks_paths_01 = sorted(list(paths.list_images(path_montgomery+folders[1])))\nright_masks_paths_01 = sorted(list(paths.list_images(path_montgomery+folders[2])))\n\nmasks_paths_01 = [left+\" \"+right for left,right in zip(left_masks_paths_01,right_masks_paths_01)]\n","metadata":{"id":"aESIiHC8kL9-","execution":{"iopub.status.busy":"2023-02-05T19:19:57.377738Z","iopub.execute_input":"2023-02-05T19:19:57.378426Z","iopub.status.idle":"2023-02-05T19:19:57.470945Z","shell.execute_reply.started":"2023-02-05T19:19:57.378386Z","shell.execute_reply":"2023-02-05T19:19:57.469999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extraindo apenas os paths das imagens -- Dataset da China\npath_chinaset = \"/kaggle/input/lungs-classification/ChinaSet_AllFiles/ChinaSet_AllFiles\"\nfolders = ['/CXR_png','/ManualMask']\n\nimgs_paths_02 = sorted(list(paths.list_images(path_chinaset+folders[0])))\nmasks_paths_02 = sorted(list(paths.list_images(path_chinaset+folders[1])))\n\n# Nem todas as imagens de Raio-X tem mask, necessário filtrar\n\nmasks_numbers = [int(re.findall(\"\\d{4}\",p)[0]) for p in masks_paths_02] # Extraindo o numero de cada mask\ntotal_numbers = np.arange(1,663,1)\nflag_array = np.array([True if n in masks_numbers else False for n in total_numbers]) # Analisando se o numero da imagem e está no total de imagens\nimgs_paths_02_new = np.array(imgs_paths_02)[flag_array].tolist() # Filtrando as imagens\n","metadata":{"id":"kyXsjOI3tfd4","execution":{"iopub.status.busy":"2023-02-05T19:19:57.473844Z","iopub.execute_input":"2023-02-05T19:19:57.474570Z","iopub.status.idle":"2023-02-05T19:19:57.713615Z","shell.execute_reply.started":"2023-02-05T19:19:57.474530Z","shell.execute_reply":"2023-02-05T19:19:57.712591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Augmentation","metadata":{"id":"7t_IqKqdbRBg"}},{"cell_type":"code","source":"# Criação das funções de aumento de dados(transformações)\n# As imagens ja estão com 224px x 224px, a fim de poupar espaço de armazenamento\ndef rotation(img,width,height,angle):\n  image_center = (int(width/2),int(height/2))\n\n  rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n\n  img = cv2.warpAffine(img, rot_mat, (int(width),int(height)))\n  img = cv2.resize(img,(640,640)) \n  return img\n\ndef croping(img,width,height,factor=0.6):\n  alpha = factor**(1/2)\n  x = int((width/2)*(1-alpha))\n  y = int((height/2)*(1-alpha))\n\n  img = img[x:-x, y:-y]\n  img = cv2.resize(img,(640,640)) \n  return img\n","metadata":{"id":"d-WGYCgxbQeg","execution":{"iopub.status.busy":"2023-02-05T12:49:07.495212Z","iopub.status.idle":"2023-02-05T12:49:07.495978Z","shell.execute_reply.started":"2023-02-05T12:49:07.495717Z","shell.execute_reply":"2023-02-05T12:49:07.495742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Criando e salvando as imagens modificadas\nda_mask_path = \"/content/data_augmented/MASKS/\"\nda_rximg_path = \"/content/data_augmented/RXIMG/\"\n\nfor i in range(len(imgs_paths_02_new)):\n  angle = np.random.randint(low=-45,high=45) # Selecionando, de forma aleatoria, um angulo para rotacionar cada par (raiox,mask)\n\n  # Augmentation nas imagens de Raio-X\n  img_path = imgs_paths_02_new[i]\n  name_rximg = img_path.split('/')[-1]\n  img = cv2.imread(img_path,cv2.IMREAD_GRAYSCALE)\n  width_01 = img.shape[0]\n  height_01 = img.shape[1]\n\n  rx_rotated = rotation(img,width_01,height_01,angle)\n  rx_croped = croping(img,width_01,height_01)\n\n  cv2.imwrite(da_rximg_path+'rotated_'+name_rximg, rx_rotated)\n  cv2.imwrite(da_rximg_path+'croped_'+name_rximg, rx_croped)\n\n  # Augmentation nas imagens das masks\n  mask_path = masks_paths_02[i]\n  name_mask = mask_path.split('/')[-1]\n\n  mask = cv2.imread(mask_path,cv2.IMREAD_GRAYSCALE)\n  width_02 = mask.shape[0]\n  height_02 = mask.shape[1]\n\n  mask_rotated = rotation(mask,width_02,height_02,angle)\n  mask_croped = croping(mask,width_02,height_02)\n\n  cv2.imwrite(da_mask_path+'rotated_'+name_mask, mask_rotated)\n  cv2.imwrite(da_mask_path+'croped_'+name_mask, mask_croped)","metadata":{"id":"tmjuLKldn1Ey","execution":{"iopub.status.busy":"2023-02-05T12:49:07.497383Z","iopub.status.idle":"2023-02-05T12:49:07.498124Z","shell.execute_reply.started":"2023-02-05T12:49:07.497865Z","shell.execute_reply":"2023-02-05T12:49:07.497890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Zipando a pasta \nmake_archive(\"data_augmented\",\"zip\",\"/kaggle/working/data_augmented\")","metadata":{"id":"jgit4i9lrgcQ","outputId":"77c9ea82-2198-4cb2-d852-1df59e5bcc9c","execution":{"iopub.status.busy":"2023-02-05T12:49:07.499507Z","iopub.status.idle":"2023-02-05T12:49:07.500282Z","shell.execute_reply.started":"2023-02-05T12:49:07.499987Z","shell.execute_reply":"2023-02-05T12:49:07.500019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Criando o dataset pytorch","metadata":{"id":"tgw2HKXf1drV"}},{"cell_type":"code","source":"# Extraindo apenas os paths das imagens -- Dataset augmented\npath_aug = \"/kaggle/input/lungs-classification/data_augmented\" \nfolders = ['/RXIMG','/MASKS']\n\nimgs_paths_03 = sorted(list(paths.list_images(path_aug+folders[0])))\nmasks_paths_03 = sorted(list(paths.list_images(path_aug+folders[1])))","metadata":{"id":"eV3UDN2d0UZ1","execution":{"iopub.status.busy":"2023-02-05T19:19:57.714941Z","iopub.execute_input":"2023-02-05T19:19:57.715331Z","iopub.status.idle":"2023-02-05T19:19:58.986355Z","shell.execute_reply.started":"2023-02-05T19:19:57.715286Z","shell.execute_reply":"2023-02-05T19:19:58.985309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Unidos todos os dados extraidos\n\nimgs_paths = imgs_paths_01 + imgs_paths_02_new + imgs_paths_03\nmasks_paths = masks_paths_01 + masks_paths_02 + masks_paths_03","metadata":{"id":"tjlqC5grblzy","execution":{"iopub.status.busy":"2023-02-05T19:19:58.991255Z","iopub.execute_input":"2023-02-05T19:19:58.993921Z","iopub.status.idle":"2023-02-05T19:19:59.001460Z","shell.execute_reply.started":"2023-02-05T19:19:58.993881Z","shell.execute_reply":"2023-02-05T19:19:59.000554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Separação em Treino e Validação\nseed = 42\nX_train,X_val,y_train,y_val = train_test_split(imgs_paths,masks_paths,\n                                               train_size=1480,random_state = seed) # dataset de treino precisa ser multiplo do batch_size\n\nprint(f'shape X_train: {len(X_train)}')\nprint(f'shape y_train: {len(y_train)}')\nprint(f'shape X_val: {len(X_val)}')\nprint(f'shape y_val: {len(y_val)}')","metadata":{"id":"lvBXADcC0OOU","outputId":"25d6ac11-6c13-4942-969a-accba2602b3b","execution":{"iopub.status.busy":"2023-02-05T19:19:59.006273Z","iopub.execute_input":"2023-02-05T19:19:59.008989Z","iopub.status.idle":"2023-02-05T19:19:59.024768Z","shell.execute_reply.started":"2023-02-05T19:19:59.008951Z","shell.execute_reply":"2023-02-05T19:19:59.023625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Definição da classe para criação do dataset pytorch personalidade\n# Utilização de transformações: Escala Cinza; Valor do pixel de 0 a 1; Transformção em Tensor; Resize em 640px x 640px\nclass RXLungDataset(torch.utils.data.Dataset):\n    def __init__(self, imgs_paths, masks_paths,clahe,data_transforms=None):\n        self.imgs_paths = imgs_paths\n        self.masks_paths = masks_paths\n        self.transform = data_transforms\n        self.clahe = clahe\n    def __getitem__(self, index):\n        \n        imgpath = self.imgs_paths[index]\n\n        rx_image = cv2.imread(imgpath,cv2.IMREAD_GRAYSCALE)\n        rx_image = clahe.apply(rx_image)/255\n        if self.transform:\n          rx_image = self.transform(rx_image)\n\n        if(self.masks_paths):\n          unpack = self.masks_paths[index].split()\n          if(len(unpack)==2): # Máscaras do dataset do Montgomery apresentam os pulmões( direito e esquerdo ) em arquivos separadas\n            leftmask_path,rightmask_path = unpack\n    \n            left_mask = cv2.imread(leftmask_path,cv2.IMREAD_GRAYSCALE)/255\n            right_mask = cv2.imread(rightmask_path,cv2.IMREAD_GRAYSCALE)/255\n            mask = left_mask+right_mask # União das máscaras dos pulmoes em uma só imagem\n          else:\n            mask = cv2.imread(self.masks_paths[index],cv2.IMREAD_GRAYSCALE)/255\n\n          if self.transform:\n            mask = self.transform(mask)\n\n          sample = (rx_image, mask)\n          return sample\n        else:\n          return rx_image\n\n    def __len__(self):\n        return len(self.imgs_paths)\n\nwidth = height = 640\ndata_transforms = transforms.Compose([transforms.ToTensor(),\n                                      transforms.Resize((width,height)),\n                                    ])\nclahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n","metadata":{"id":"ZeBfApyMlrkI","execution":{"iopub.status.busy":"2023-02-05T19:19:59.032795Z","iopub.execute_input":"2023-02-05T19:19:59.033066Z","iopub.status.idle":"2023-02-05T19:19:59.054269Z","shell.execute_reply.started":"2023-02-05T19:19:59.033041Z","shell.execute_reply":"2023-02-05T19:19:59.053215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Criação dos dataset de treino e validação\ntrain_dataset = RXLungDataset(X_train,y_train,clahe,data_transforms)\nval_dataset = RXLungDataset(X_val,y_val,clahe,data_transforms)\n\nbatch_size = 8\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\nval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False)","metadata":{"id":"pztccfUxFDU_","execution":{"iopub.status.busy":"2023-02-05T19:19:59.058823Z","iopub.execute_input":"2023-02-05T19:19:59.061163Z","iopub.status.idle":"2023-02-05T19:19:59.069143Z","shell.execute_reply.started":"2023-02-05T19:19:59.061098Z","shell.execute_reply":"2023-02-05T19:19:59.068205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualização de alguns pares (x,y)\nfor step, (x, y) in enumerate(train_dataset):\n  plt.subplot(1, 2, 1)\n  plt.imshow(x.detach().numpy().reshape((width,height)),cmap='gray')\n  plt.subplot(1, 2, 2)\n  plt.imshow(y.detach().numpy().reshape((width,height)),cmap='gray')\n  plt.show()\n  if(step==5):\n    break","metadata":{"id":"NYVXpt0RpeKn","outputId":"bee51c55-b8e0-4bfb-ae58-28e4fd852e9e","execution":{"iopub.status.busy":"2023-02-05T14:05:07.487838Z","iopub.execute_input":"2023-02-05T14:05:07.488865Z","iopub.status.idle":"2023-02-05T14:05:10.681893Z","shell.execute_reply.started":"2023-02-05T14:05:07.488827Z","shell.execute_reply":"2023-02-05T14:05:10.680873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2-) Modelo","metadata":{"id":"v6ywa2AeczHE"}},{"cell_type":"code","source":"# Criação da classe para o modelo personalizado utilizando o framework Lightning torch\nclass LungModel(pl.LightningModule):\n    def __init__(self,arch, encoder_name, in_channels, out_classes,batch_size):\n        super().__init__()\n        # Importação do modelo com encode ja pretreinado\n        self.model = sm.create_model(\n            arch, encoder_name=encoder_name, in_channels=in_channels, classes=out_classes)\n        \n        self.in_channels = in_channels\n        self.batch_size = batch_size\n        self.loss_fn = sm.losses.DiceLoss(sm.losses.BINARY_MODE, from_logits=True)\n\n    def forward(self, image):\n        return self.model(image)\n\n    def shared_step(self, batch, stage):\n        image,mask = batch\n\n        image = image.unsqueeze(0) \n        mask = mask.unsqueeze(0)\n\n        image = torch.reshape(image,[self.batch_size, self.in_channels, width,height])\n        mask = torch.reshape(mask,[self.batch_size, self.in_channels, width,height])\n\n        y_hat = self.forward(image)\n\n        loss = self.loss_fn(y_hat, mask)\n\n        # Utilização da função sigmoid para os valores ficar entre 0 e 1\n        prob_mask = y_hat.sigmoid() \n        # Toda predição abaixo de 0.5(50%) é dada como fora dos pulmões, o contrario ocorre para aquelas acima de 0.5\n        pred_mask = (prob_mask > 0.5) \n\n        tp, fp, fn, tn = sm.metrics.get_stats(pred_mask.long(), mask.long(), mode=\"binary\")\n\n        return {\n            \"loss\": loss,\n            \"tp\": tp,\n            \"fp\": fp,\n            \"fn\": fn,\n            \"tn\": tn,\n        }\n    def shared_epoch_end(self, outputs, stage):\n        # Criação de métrica de avaliação\n        tp = torch.cat([x[\"tp\"] for x in outputs])\n        fp = torch.cat([x[\"fp\"] for x in outputs])\n        fn = torch.cat([x[\"fn\"] for x in outputs])\n        tn = torch.cat([x[\"tn\"] for x in outputs])\n\n     \n        per_image_iou = sm.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n        \n        dataset_iou = sm.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n\n        metrics = {\n            f\"{stage}_per_image_iou\": per_image_iou,\n            f\"{stage}_dataset_iou\": dataset_iou,\n        }\n        \n        self.log_dict(metrics, prog_bar=True)\n\n    def training_step(self, batch, batch_idx):\n        return self.shared_step(batch, \"train\") \n\n    def validation_step(self, batch, batch_idx):\n        return self.shared_step(batch, \"valid\")\n\n    def validation_epoch_end(self, outputs):\n        return self.shared_epoch_end(outputs, \"train\")\n\n    def validation_epoch_end(self, outputs):\n        return self.shared_epoch_end(outputs, \"valid\")\n\n    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n        return self(batch)\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=0.0001)","metadata":{"id":"35UPWLE_5PaP","execution":{"iopub.status.busy":"2023-02-05T19:19:59.077322Z","iopub.execute_input":"2023-02-05T19:19:59.077672Z","iopub.status.idle":"2023-02-05T19:19:59.101993Z","shell.execute_reply.started":"2023-02-05T19:19:59.077639Z","shell.execute_reply":"2023-02-05T19:19:59.100607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instaciamento do modelo, definindo arquitetura e backbone\nmodel = LungModel(\"Unet\",\"resnet34\",1,1,batch_size)\nmodel = model.double()","metadata":{"id":"DKZaL9Z4RzAd","outputId":"becf6a43-1d10-4770-d6e8-6dde8df1099e","execution":{"iopub.status.busy":"2023-02-05T14:05:41.152710Z","iopub.execute_input":"2023-02-05T14:05:41.153092Z","iopub.status.idle":"2023-02-05T14:05:46.996345Z","shell.execute_reply.started":"2023-02-05T14:05:41.153060Z","shell.execute_reply":"2023-02-05T14:05:46.995237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Criação do objeto de treino\ntrainer = pl.Trainer(\n    gpus=1,\n    max_epochs=5\n)","metadata":{"id":"8jP9HjfO8xAQ","outputId":"af793b92-7b7e-496f-a717-e5e5849e484d","execution":{"iopub.status.busy":"2023-02-05T14:05:50.111910Z","iopub.execute_input":"2023-02-05T14:05:50.112341Z","iopub.status.idle":"2023-02-05T14:05:50.504858Z","shell.execute_reply.started":"2023-02-05T14:05:50.112306Z","shell.execute_reply":"2023-02-05T14:05:50.503129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finetuning o modelo\ntrainer.fit(\n    model, \n    train_dataloaders=train_dataloader,\n)","metadata":{"id":"grEqkESNZuGh","outputId":"8071b887-8e6d-4b6d-ea87-771946b291ec","execution":{"iopub.status.busy":"2023-02-05T14:05:53.606072Z","iopub.execute_input":"2023-02-05T14:05:53.606429Z","iopub.status.idle":"2023-02-05T15:31:57.535333Z","shell.execute_reply.started":"2023-02-05T14:05:53.606399Z","shell.execute_reply":"2023-02-05T15:31:57.534310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path = \"/kaggle/working/\" + \"UnetResNet_LungSegmentation\"","metadata":{"id":"rrq1FYG_RQQ2","execution":{"iopub.status.busy":"2023-02-05T18:50:47.118155Z","iopub.execute_input":"2023-02-05T18:50:47.118759Z","iopub.status.idle":"2023-02-05T18:50:47.124748Z","shell.execute_reply.started":"2023-02-05T18:50:47.118709Z","shell.execute_reply":"2023-02-05T18:50:47.123697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), model_path)","metadata":{"id":"iMBp3qaKMgau","execution":{"iopub.status.busy":"2023-02-05T15:36:51.293669Z","iopub.execute_input":"2023-02-05T15:36:51.294066Z","iopub.status.idle":"2023-02-05T15:36:51.808540Z","shell.execute_reply.started":"2023-02-05T15:36:51.294031Z","shell.execute_reply":"2023-02-05T15:36:51.805725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3-) Avaliação","metadata":{"id":"_2k9iQPHQ2CE"}},{"cell_type":"code","source":"# Importação do modelo finetuned\nmodel_path = \"/kaggle/input/lungs-classification/UnetResNet_LungSegmentation\"\nmodel = LungModel(\"Unet\",\"resnet34\",1,1,1)\nmodel.load_state_dict(torch.load(model_path))\nmodel.double()","metadata":{"id":"0slqjBnZRBxI","outputId":"3c791fb0-bedd-472f-cc80-248eaf4fcb86","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = pl.Trainer(\n    gpus=1,\n    max_epochs=5\n)","metadata":{"id":"guSUNKZygHUM","outputId":"1b42d050-5ecd-41e3-dfdd-faf8e616c2e3","execution":{"iopub.status.busy":"2023-02-05T19:20:01.939128Z","iopub.execute_input":"2023-02-05T19:20:01.940225Z","iopub.status.idle":"2023-02-05T19:20:02.115641Z","shell.execute_reply.started":"2023-02-05T19:20:01.940182Z","shell.execute_reply":"2023-02-05T19:20:02.114570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Avaliação do modelo\nvalid_metrics = trainer.validate(model, dataloaders=val_dataloader, verbose=False)\nprint(valid_metrics)","metadata":{"id":"-qn63fF-NK6Z","outputId":"a7831aa1-e101-4999-940d-09045a1a68cf","execution":{"iopub.status.busy":"2023-02-05T15:38:44.730794Z","iopub.execute_input":"2023-02-05T15:38:44.731391Z","iopub.status.idle":"2023-02-05T15:39:44.335815Z","shell.execute_reply.started":"2023-02-05T15:38:44.731353Z","shell.execute_reply":"2023-02-05T15:39:44.334662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4-) Predição","metadata":{"id":"U7NH3Bs8XIWD"}},{"cell_type":"code","source":"# Definição dos diretorios onde estão os dados\ntest_train_path = \"/kaggle/input/coronahack-chest-xraydataset/Coronahack-Chest-XRay-Dataset/Coronahack-Chest-XRay-Dataset/train\"\ntest_train_paths = sorted(list(paths.list_images(test_train_path)))\n\ntest_test_path = \"/kaggle/input/coronahack-chest-xraydataset/Coronahack-Chest-XRay-Dataset/Coronahack-Chest-XRay-Dataset/test\"\ntest_test_paths = sorted(list(paths.list_images(test_test_path)))","metadata":{"id":"f9otMd75n7OX","execution":{"iopub.status.busy":"2023-02-05T19:50:01.115142Z","iopub.execute_input":"2023-02-05T19:50:01.115576Z","iopub.status.idle":"2023-02-05T19:50:02.996575Z","shell.execute_reply.started":"2023-02-05T19:50:01.115544Z","shell.execute_reply":"2023-02-05T19:50:02.995226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Segmentação das imagens que irão para classificação -- Predição e salvamento das imagens feito em \"batchs\" de 400 para economizar RAM\nimport os\nos.makedirs('/kaggle/working/lungs/train')\nos.makedirs('/kaggle/working/lungs/test')\nlungs_test_path = \"/kaggle/working/lungs/\"\n\ndef make_predictions(paths,f):\n    for j in range(0,int(len(paths)/400)+1):\n        path_slice = paths[j*400:min((j+1)*400,len(paths))]\n        dataset = RXLungDataset(path_slice,None,clahe,data_transforms)\n        dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n        predictions = trainer.predict(model, test_train_dataloader)\n    \n        for i in range(len(predictions)):\n\n            image = dataset[i]\n            mask = predictions[i]\n\n            mask = mask.detach().numpy().reshape((width,height))\n            image = image.detach().numpy().reshape((width,height))\n\n            mask[mask>=0.5] = 1\n            mask[mask<0.5] = 0\n            image[mask==0] = 0\n\n            lung = PIL.Image.fromarray(image*255)\n            lung = lung.convert('RGB')\n            name = path_slice[i].split('/')[-1]\n            lung.save(lungs_test_path+f+name)\n\nmake_predictions(test_train_paths,'train/') \nmake_predictions(test_test_paths,'test/')","metadata":{"id":"BmoPEnP0iM8U","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"some_test_imags = lungs_test_path+'train/'\nfor lung_path in sorted(list(paths.list_images(some_test_imags)))[:5]:\n  lung = cv2.imread(lung_path)\n  plt.imshow(lung)\n  plt.show()","metadata":{"id":"qoK08iY9eJgQ","outputId":"5f791ff0-847b-41bf-fd9d-2fbfafb6d682","execution":{"iopub.status.busy":"2023-02-05T19:58:43.580352Z","iopub.execute_input":"2023-02-05T19:58:43.581432Z","iopub.status.idle":"2023-02-05T19:58:44.983123Z","shell.execute_reply.started":"2023-02-05T19:58:43.581390Z","shell.execute_reply":"2023-02-05T19:58:44.982153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Zipando a pasta \nmake_archive(\"lungs_classification\",\"zip\",\"/kaggle/working/lungs\")","metadata":{"id":"v7FVw6q7cpw8","outputId":"7a23d2d3-31b5-40f5-c55d-3ac0dda4f60f","execution":{"iopub.status.busy":"2023-02-05T19:55:05.164734Z","iopub.execute_input":"2023-02-05T19:55:05.165140Z","iopub.status.idle":"2023-02-05T19:55:12.207508Z","shell.execute_reply.started":"2023-02-05T19:55:05.165107Z","shell.execute_reply":"2023-02-05T19:55:12.206431Z"},"trusted":true},"execution_count":null,"outputs":[]}]}